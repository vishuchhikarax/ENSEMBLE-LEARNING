{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PW SKILLS**\n",
        "## ENSEMBLE LEARNING ASSIGNMENT"
      ],
      "metadata": {
        "id": "JTLMyUhUc4DO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1**: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "**Answer:**\n",
        "Ensemble Learning is a powerful technique in machine learning where multiple models (often called “weak learners”) are combined to produce a stronger predictive model. The central idea is that instead of relying on a single model, combining the predictions of multiple models helps reduce errors, improve accuracy, and generalize better to unseen data.\n",
        "\n",
        "The logic behind ensemble learning is similar to the concept of “wisdom of the crowd.” Just as collective decisions made by a group are often more reliable than an individual’s judgment, the combined predictions of several models usually perform better than a single model.\n",
        "\n",
        "Key advantages of ensemble learning include:\n",
        "\n",
        "Reduction of variance – Combining models reduces overfitting, making predictions more stable.\n",
        "\n",
        "Reduction of bias – By combining weak learners, the overall bias is reduced, leading to better accuracy.\n",
        "\n",
        "Improved generalization – Ensembles generalize better on test data.\n",
        "\n",
        "Common ensemble techniques are Bagging (Bootstrap Aggregating), Boosting, and Stacking. For example, Random Forest uses bagging with decision trees, while AdaBoost and XGBoost use boosting techniques.\n",
        "\n",
        "Thus, the key idea of ensemble learning is: “A group of weak learners, when combined in the right way, can create a strong learner that outperforms any individual model.”"
      ],
      "metadata": {
        "id": "j8Wrvl2rdFqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 2: What is the difference between Bagging and Boosting?**\n",
        "\n",
        "Answer:\n",
        "Bagging and Boosting are two major ensemble learning techniques, but they differ in approach and objectives:\n",
        "\n",
        "Definition:\n",
        "\n",
        "Bagging (Bootstrap Aggregating): Creates multiple subsets of the dataset using bootstrap sampling, trains separate models on each subset, and combines predictions (e.g., Random Forest).\n",
        "\n",
        "Boosting: Builds models sequentially, where each new model focuses on correcting the errors of the previous one (e.g., AdaBoost, XGBoost).\n",
        "\n",
        "Training Approach:\n",
        "\n",
        "Bagging trains models in parallel.\n",
        "\n",
        "Boosting trains models sequentially.\n",
        "\n",
        "Error Handling:\n",
        "\n",
        "Bagging reduces variance (helps avoid overfitting).\n",
        "\n",
        "Boosting reduces bias (helps improve weak learners).\n",
        "\n",
        "Weights on Data:\n",
        "\n",
        "Bagging treats all observations equally.\n",
        "\n",
        "Boosting assigns higher weights to misclassified samples, forcing later models to focus on them.\n",
        "\n",
        "Performance:\n",
        "\n",
        "Bagging is more stable and less prone to overfitting.\n",
        "\n",
        "Boosting usually gives higher accuracy but risks overfitting if not tuned properly.\n",
        "\n",
        "Example:\n",
        "\n",
        "Random Forest → Bagging technique.\n",
        "\n",
        "AdaBoost/XGBoost → Boosting techniques.\n",
        "\n",
        "In short, Bagging focuses on stability and variance reduction, while Boosting focuses on accuracy and bias reduction."
      ],
      "metadata": {
        "id": "e9hxmep4dpX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "Answer:\n",
        "Bootstrap Sampling is a statistical technique where multiple subsets of data are created from the original dataset by sampling with replacement. This means the same data point can appear multiple times in one subset, while some data points may not appear at all.\n",
        "\n",
        "In Bagging methods (like Random Forest):\n",
        "\n",
        "Each base learner (e.g., a decision tree) is trained on a different bootstrap sample of the dataset.\n",
        "\n",
        "Because each learner sees slightly different data, the models become diverse.\n",
        "\n",
        "The final prediction is obtained by aggregating (majority vote for classification or averaging for regression).\n",
        "\n",
        "Why it is important in Random Forest:\n",
        "\n",
        "Ensures diversity among decision trees.\n",
        "\n",
        "Reduces correlation between models.\n",
        "\n",
        "Leads to better generalization and reduced overfitting.\n",
        "\n",
        "For example, in Random Forest, if we create 100 trees, each tree is trained on a bootstrap sample of the dataset. This makes the ensemble stronger and more robust compared to a single decision tree."
      ],
      "metadata": {
        "id": "HHr00MWadvcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question** 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Answer:\n",
        "When bootstrap sampling is performed in Bagging methods, some data points are left out of the training subset. These are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "Key points:\n",
        "\n",
        "On average, about 36% of data is not included in a bootstrap sample.\n",
        "\n",
        "These OOB samples can act as a validation set for the model.\n",
        "\n",
        "Each base learner (e.g., a tree in Random Forest) is tested on its corresponding OOB samples.\n",
        "\n",
        "OOB Score:\n",
        "\n",
        "It is the performance of the model evaluated using only the OOB samples.\n",
        "\n",
        "Provides an unbiased estimate of the generalization error without needing a separate validation/test set.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Saves computation time and data (no need for separate cross-validation).\n",
        "\n",
        "Gives a reliable performance measure during model training.\n",
        "\n",
        "For example, in a Random Forest classifier, the OOB score is often used as a quick evaluation metric to judge accuracy."
      ],
      "metadata": {
        "id": "mgGxGgefd478"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        "Answer:\n",
        "In a Single Decision Tree:\n",
        "\n",
        "Feature importance is calculated based on how much a feature decreases impurity (e.g., Gini impurity or entropy) when used for splitting.\n",
        "\n",
        "The importance value depends heavily on the structure of that specific tree.\n",
        "\n",
        "It can be biased, as one strong feature may dominate the tree structure, while others may be ignored.\n",
        "\n",
        "In a Random Forest (Ensemble of Trees):\n",
        "\n",
        "Feature importance is averaged across multiple decision trees.\n",
        "\n",
        "Each feature’s importance is measured by its contribution to reducing impurity in all trees.\n",
        "\n",
        "Provides a more stable and reliable ranking compared to a single tree.\n",
        "\n",
        "Reduces bias since multiple trees contribute to the calculation.\n",
        "\n",
        "Comparison:\n",
        "\n",
        "Single Tree → Importance may be unstable and highly dataset-dependent.\n",
        "\n",
        "Random Forest → More robust, generalized, and reliable measure of importance.\n",
        "\n",
        "Thus, Random Forest provides a better and more balanced estimate of feature importance than a single Decision Tree."
      ],
      "metadata": {
        "id": "iBqHVUEjd94F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 6: Python program – Random Forest on Breast Cancer dataset & top 5 important features**\n",
        "\n",
        "Answer:\n",
        "Random Forest is an ensemble of decision trees that ranks features by their contribution to classification accuracy. Using the Breast Cancer dataset, we can identify which features are most important for predicting whether a tumor is malignant or benign."
      ],
      "metadata": {
        "id": "uDsetHjgeFwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 6:\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 important features\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikO8i1D2dAKN",
        "outputId": "9e54a0fa-4623-41e6-e329-875aa33f0dec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 7: Python program – Bagging Classifier vs Decision Tree on Iris dataset**\n",
        "\n",
        "Answer:\n",
        "Bagging improves model performance by combining multiple Decision Trees trained on bootstrapped samples. We compare its accuracy with a single Decision Tree."
      ],
      "metadata": {
        "id": "x1yc8EBPeczs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 7:\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Bagging with Decision Trees\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                            n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CkO3634ee0t",
        "outputId": "cd396fd1-7cfd-4869-b3c1-7f9ecae33f5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question **8**: Python program – Random Forest with GridSearchCV for hyperparameter tuning\n",
        "\n",
        "Answer:\n",
        "Random Forest performance depends on hyperparameters like max_depth and n_estimators. GridSearchCV helps find the best combination."
      ],
      "metadata": {
        "id": "QRscB6Qhelkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 8\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,\n",
        "                                                    test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 10, None]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "final_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", final_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj0OJl1oetm0",
        "outputId": "a0372107-7ea1-442e-c332-596bd411462c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 10, 'n_estimators': 200}\n",
            "Final Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 9: Python program – Bagging Regressor vs Random Forest Regressor (California Housing dataset)**\n",
        "\n",
        "Answer:\n",
        "For regression, ensemble methods like Bagging and Random Forest reduce prediction errors. We compare their Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "ubF4r_7vfXID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 9\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,\n",
        "                                                    test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(n_estimators=50, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "y_pred_bag = bag_reg.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", mse_bag)\n",
        "print(\"Random Forest Regressor MSE:\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ROSEEQDfdIW",
        "outputId": "75c58afd-2fc6-42a5-9837-e83997e704f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25787382250585034\n",
            "Random Forest Regressor MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 10: Real-world case – Ensemble Learning for Loan Default Prediction**\n",
        "\n",
        "Answer:\n",
        "In a financial institution, predicting loan default is crucial. Using ensemble methods improves reliability and reduces risk.\n",
        "\n",
        "Step-by-Step Approach:\n",
        "\n",
        "Choice between Bagging or Boosting:\n",
        "\n",
        "If the dataset is large and prone to overfitting → Bagging (Random Forest).\n",
        "\n",
        "If accuracy and handling complex relationships matter → Boosting (XGBoost).\n",
        "\n",
        "Handling Overfitting:\n",
        "\n",
        "Use techniques like limiting tree depth (max_depth), early stopping, and regularization.\n",
        "\n",
        "Cross-validation to tune parameters.\n",
        "\n",
        "Selecting Base Models:\n",
        "\n",
        "Decision Trees are common base learners.\n",
        "\n",
        "Logistic Regression + Gradient Boosting can also be combined in stacking.\n",
        "\n",
        "Performance Evaluation:\n",
        "\n",
        "Use k-fold cross-validation to measure accuracy, precision, recall, and ROC-AUC.\n",
        "\n",
        "OOB score can be used in Random Forest.\n",
        "\n",
        "Justification of Ensemble Learning in Banking:\n",
        "\n",
        "Reduces risk of wrong predictions.\n",
        "\n",
        "Improves decision-making by combining strengths of multiple models.\n",
        "\n",
        "Ensures fairness and stability in loan approval systems."
      ],
      "metadata": {
        "id": "7Abad9GEfiyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 10:\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Simulated dataset (replace with real banking data)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Gradient Boosting (Boosting method)\n",
        "gb = GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "print(\"Loan Default Prediction Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl9T5w7Dfsgu",
        "outputId": "75c1f453-1d8c-4820-89e2-2610a2d55726"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loan Default Prediction Accuracy: 0.8833333333333333\n"
          ]
        }
      ]
    }
  ]
}